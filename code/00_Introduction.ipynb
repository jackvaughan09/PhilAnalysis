{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the Phil Analysis Project\n",
    "\n",
    "The data for this project comes from a data extraction/engineering application I developed here: [PhilAuditSystem](https://github.com/jackvaughan09/PhilippinesAuditSystem)\n",
    "\n",
    "I'll give a synopsis of what is going on over there to develop the context for the analysis conducted in the following notebooks.\n",
    "\n",
    "Political and economic development researcher [Mike Denly](https://mikedenly.com/) has been studying codified corruption in the governments of the developing world for a number of years. His current work focuses on quantifying corrupt practices via a novel methodology of using internal government audit reports to detect underhand dealings and legal gymnastics. This practice [works exceptionally well in some places](https://mikedenly.com/research/audit-measurement), and is why I was recruited to accomplish the task given here!\n",
    "\n",
    "In late 2022, we discovered a treasure trove of audit reports for the government of the Philippines [posted online](https://www.coa.gov.ph/reports/annual-audit-reports/aar-local-government-units/#167-428-leyte). Unfortunately, the site is guarded against scraping with cloudflare technology, so we've had to download the some 12,000 reports manually in the form of zip files. \n",
    "\n",
    "Each zip file contains some file structure that looks like this, for example:\n",
    "```\n",
    "├── MunicipalityYearAuditReport\n",
    "│   ├── Auditors_reportYear.docx\n",
    "│   ├── AuditCertificate.doc\n",
    "│   ├── AuditStatus-report-year.pdf\n",
    "│   ├── etc.\n",
    "└── etc.\n",
    "```\n",
    "Inside of files like `Auditors_reportYear.docx` and `AuditStatus-report-year.pdf` are tables containing lists of observations.  \n",
    "\n",
    "\n",
    "### Step by step, the audit extraction system:\n",
    "\n",
    "**I.** Unzips all zip files input to the software\n",
    "\n",
    "**II.** Filters through the slew of unwanted files / Detects only the audit reports\n",
    "\n",
    "**III.** Converts all relevant files to a standard PDF format (all .doc/.docx files and non-standard PDFs) using Linux-based [LibreOffice](https://www.libreoffice.org/) command line tools and shell scripting.\n",
    "\n",
    "**IV.** Scrapes the PDF:\n",
    "   1. Opens the PDF with `PyPDF2` \n",
    "   2. Locates the relevant portion of the document containing tables with fuzzy logic. \n",
    " \n",
    "\n",
    "   3. Uses a computer vision library, [camelot](https://camelot-py.readthedocs.io/en/master/) to extract the tables containing the audit observations\n",
    "   4. Implements an array of hand-crafted data cleaning tools to filter out bad data and consolidate good observations. For each document it:\n",
    "   \n",
    "      *i.* Establishes canonical headers for the document so that each table from the document can be joined in a single dataframe. \n",
    "\n",
    "         - some tables do not have headers (column names) and get missed for this reason\n",
    "\n",
    "         - some tables have extra headers that become entrenched in the observation space\n",
    "\n",
    "      *ii.* Filters out rows of entrenched headers\n",
    "\n",
    "      *iii.* Attempts to coerce document headers into a standard set for the document corpus by using fuzzy logic to find a \"good match\" in the canon headers for each column name. We do this so that all tables from all documents passed to the system can be joined in a single file at the end (this requires standard column names)\n",
    "\n",
    "            \n",
    "            CANON_HEADERS = [\n",
    "               \"audit observation\",\n",
    "               \"recommendations\",\n",
    "               \"references\",\n",
    "               \"status of implementation\",\n",
    "               \"reasons for partial/non-implementation\",\n",
    "               \"management action\"\n",
    "            ]            \n",
    "\n",
    "      *iv.* Conducts overflow repair:\n",
    "\n",
    "         - Some documents have table rows that span multiple pages, so some observations are cut in half. \n",
    "        \n",
    "         - I use a number of learned rules from research on the corpus to locate these rows and concatenate them back with the originating row\n",
    "        \n",
    "      *v.* Tags each observation with the source document name\n",
    "      \n",
    "      *vi.* Compiles all scraped observations into a dataframe <br>\n",
    "   <br>   \n",
    "\n",
    "**V.** Compiles all document dataframes to a single .xlsx file\n",
    "\n",
    "**VI.** Returns collated data, the converted pdfs, and a log of filtering/data loss from the extraction process\n",
    "\n",
    "\n",
    "> The entire program runs inside of a Docker container that interacts with the host filesystem to load and return data. \n",
    "\n",
    "### System Diagram:\n",
    "\n",
    "![System Diagram](../img/introduction/auditsystemdiagram.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without further ado...\n",
    "\n",
    "> Let's get into the analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
